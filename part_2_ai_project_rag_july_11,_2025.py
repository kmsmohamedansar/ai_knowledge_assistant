# -*- coding: utf-8 -*-
"""Part 2 AI project - RAG - July 11, 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GB7BuPaW-NSg40u5qFGOsJV4lnRdy0lA
"""

!pip install -q langchain langchain-community faiss-cpu sentence-transformers transformers accelerate gradio kagglehub

"""Explanation:

We import the core libraries we will use for handling files, data, and dataset download.

kagglehub.dataset_download() simplifies downloading and extracting the dataset so we don't have to do it manually.

Finally, we check the contents of the downloaded folder to confirm all files are present.
"""

# Import OS module to handle environment variables and file paths
import os

# Pandas for data manipulation and analysis
import pandas as pd

# Kagglehub to download datasets from Kaggle easily within Colab
import kagglehub

# Print statements to check progress and debug easily
print("Starting dataset download...")

# Download the Instacart Grocery dataset from Kaggle using kagglehub
# This downloads and extracts the dataset, returning the local path
data_path = kagglehub.dataset_download("yasserh/instacart-online-grocery-basket-analysis-dataset")

print(f"Dataset downloaded and extracted at: {data_path}")

# List the files inside the dataset directory to verify contents
print("Files in the dataset directory:")
print(os.listdir(data_path))

"""Detailed comments:

We specify the folder where the dataset lives (using the path you got from the previous step).

Use pd.read_csv() to load each CSV into a pandas DataFrame, which is like an Excel table for Python.

We load three CSVs:

products.csv: List of products with product IDs, names, aisle and department IDs.

aisles.csv: List of aisles with aisle IDs and names.

departments.csv: List of departments with department IDs and names.

Finally, we print a preview of the first 5 rows of each table so we can check what the data looks like.
"""

# Build the path to the dataset folder
data_path = "/root/.cache/kagglehub/datasets/yasserh/instacart-online-grocery-basket-analysis-dataset/versions/1"

# Load the main CSV files using pandas
products_df = pd.read_csv(os.path.join(data_path, "products.csv"))
aisles_df = pd.read_csv(os.path.join(data_path, "aisles.csv"))
departments_df = pd.read_csv(os.path.join(data_path, "departments.csv"))

# Show the first few rows of each file to understand the structure
print("Sample products data:")
print(products_df.head())

print("\nSample aisles data:")
print(aisles_df.head())

print("\nSample departments data:")
print(departments_df.head())

"""Detailed comments:
The products table only has IDs for aisles and departments, but those IDs are not very human-friendly.

We join (merge) the products table with aisles and departments based on their IDs to get their actual names.

how="left" means we keep all products and bring in matching aisle/department names.

After merging, each product will have these extra columns: 'aisle' and 'department'.

Finally, we print a few rows to confirm the merge worked and the product rows now have aisle and department names.
"""

# Merge aisles into products so each product has aisle name instead of just aisle_id
products_full = products_df.merge(aisles_df, how="left", on="aisle_id")

# Merge departments into the result so each product also has department name
products_full = products_full.merge(departments_df, how="left", on="department_id")

# Show a sample to verify that the merges worked correctly
print("Sample merged product data with aisle and department names:")
print(products_full.head())

"""Explanation:
We define a function make_product_text that takes a row and returns a sentence combining product name, aisle, and department.

We use apply() to run this function on each row and save the result in a new column called combined_text.

This text will be used to create embeddings for semantic search later.

Printing the first example helps us verify that the combined text looks good.
"""

# Define a function to create a combined description text for each product row
def make_product_text(row):
    return f"Product: {row['product_name']}. Aisle: {row['aisle']}. Department: {row['department']}."

# Apply the function to each row in the merged dataframe and store the result in a new column
products_full["combined_text"] = products_full.apply(make_product_text, axis=1)

# Show an example of the combined text for the first product
print("Example product description:")
print(products_full["combined_text"].iloc[0])

"""Detailed Explanation:
HuggingFaceEmbeddings wraps the sentence-transformers model for easy use with LangChain.

FAISS.from_texts() converts all product descriptions into vectors and builds a fast searchable index.

Now, we have a "memory" where the AI can find products similar to any query by vector similarity.
"""

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# Load the local embedding model (this will download the model the first time)
local_embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create vector store (FAISS) from the combined product description texts
vector_store = FAISS.from_texts(products_full["combined_text"].tolist(), local_embedder)

print("âœ… Vector store created with local sentence-transformer embeddings!")

"""Explanation:
pipeline("text2text-generation", ...) loads a text generation model ready for Q&A style prompts.

The function does a similarity search over the indexed product descriptions and feeds the top ones as context.

The model tries to answer your question using that context.
"""

from transformers import pipeline

# Load the local text-to-text generation model
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base", max_length=256)

print("ðŸ§  Local LLM (flan-t5-base) loaded!")

# Define the QA function
def local_answer_question(query):
    # Step 1: Retrieve top 4 similar product descriptions
    docs = vector_store.similarity_search(query, k=4)

    # Step 2: Combine them into a context string
    context = "\n".join([doc.page_content for doc in docs])

    # Step 3: Create prompt with context and question
    prompt = f"Answer the question using the context below:\n\nContext:\n{context}\n\nQuestion: {query}"

    # Step 4: Get model's generated answer
    result = qa_pipeline(prompt)[0]["generated_text"]
    return result

"""Explanation:
fn=local_answer_question means the Gradio app will send the user input to your local QA function.

The interface is minimal: a text input box and a text output area.

share=True creates a temporary public link, useful when running in Colab.

debug=True will print any errors live in the notebook output.
"""

import gradio as gr

# Gradio interface using the local_answer_question function
chat_interface = gr.Interface(
    fn=local_answer_question,   # function to answer questions
    inputs="text",              # input type is plain text
    outputs="text",             # output will be plain text answer
    title="ðŸ›’ Grocery AI Assistant (Fully Local)",
    description="Ask about grocery products â€” answers generated using FAISS + flan-t5-base!"
)

# Launch the interface; share=True gives you a public link
chat_interface.launch(share=True, debug=True)